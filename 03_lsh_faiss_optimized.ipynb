{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-Sensitive Hashing (LSH) using FAISS (Optimized)\n",
    "\n",
    "This notebook evaluates approximate nearest neighbor search using Facebook's FAISS library with LSH indexing.\n",
    "\n",
    "## Method: FAISS IndexLSH\n",
    "FAISS (Facebook AI Similarity Search) provides highly optimized C++ implementations of various ANN methods, including LSH.\n",
    "\n",
    "**Key advantages:**\n",
    "- ✅ Production-grade C++ implementation (10-100x faster than Python)\n",
    "- ✅ Optimized for modern CPUs with SIMD instructions\n",
    "- ✅ Used by Facebook, Google, and industry leaders\n",
    "- ✅ Well-tested on billion-scale datasets\n",
    "\n",
    "## Installation\n",
    "```bash\n",
    "# CPU version (recommended for this project)\n",
    "pip install faiss-cpu\n",
    "\n",
    "# OR GPU version (if you have CUDA)\n",
    "pip install faiss-gpu\n",
    "```\n",
    "\n",
    "## Metrics\n",
    "- **Recall@10**: Proportion of true nearest neighbors retrieved\n",
    "- **Query time**: Milliseconds per query\n",
    "- **Build time**: Index construction time\n",
    "- **Memory**: Total index size in MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Import FAISS\n",
    "try:\n",
    "    import faiss\n",
    "    print(f\"✓ FAISS version: {faiss.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: FAISS not installed!\")\n",
    "    print(\"Install with: pip install faiss-cpu\")\n",
    "    raise\n",
    "\n",
    "# Import our custom modules\n",
    "sys.path.append('..')  \n",
    "from evaluator import ANNEvaluator\n",
    "from datasets import DatasetLoader, print_dataset_info\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "K = 10  # Number of nearest neighbors\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Datasets\n",
    "DATASETS = [\n",
    "    'sift',\n",
    "    'gist',\n",
    "    'deep1b'\n",
    "]\n",
    "\n",
    "# LSH parameters for FAISS\n",
    "# nbits: number of hash bits (higher = finer buckets, lower recall but faster)\n",
    "HASH_BITS_OPTIONS = [64, 128, 256, 512, 1024]  # FAISS uses more bits than custom implementation\n",
    "\n",
    "# Subset sizes\n",
    "N_TRAIN = None\n",
    "N_TEST = 1000\n",
    "\n",
    "# Results directory\n",
    "RESULTS_DIR = \"../results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  k (neighbors) = {K}\")\n",
    "print(f\"  Datasets: {DATASETS}\")\n",
    "print(f\"  LSH hash bits: {HASH_BITS_OPTIONS}\")\n",
    "print(f\"\\nNote: FAISS uses binary codes, so nbits is total bits (not tables × bits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FAISS LSH Implementation\n",
    "\n",
    "FAISS `IndexLSH` uses binary hash codes:\n",
    "- Projects vectors to binary codes of specified length\n",
    "- Uses Hamming distance between codes for fast approximate search\n",
    "- Optimized C++ implementation with SIMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISSLSHIndex:\n",
    "    \"\"\"\n",
    "    Wrapper for FAISS IndexLSH with our evaluator interface.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, nbits):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Dimension of input vectors\n",
    "            nbits: Number of hash bits (length of binary code)\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.nbits = nbits\n",
    "        \n",
    "        # Create FAISS LSH index\n",
    "        self.index = faiss.IndexLSH(dim, nbits)\n",
    "        \n",
    "    def fit(self, X_train):\n",
    "        \"\"\"Build LSH index\"\"\"\n",
    "        print(f\"  Building FAISS LSH index: {X_train.shape[0]:,} points, {self.nbits} bits\")\n",
    "        \n",
    "        # Ensure data is float32 (FAISS requirement)\n",
    "        if X_train.dtype != np.float32:\n",
    "            X_train = X_train.astype(np.float32)\n",
    "        \n",
    "        # Ensure C-contiguous (FAISS requirement)\n",
    "        if not X_train.flags['C_CONTIGUOUS']:\n",
    "            X_train = np.ascontiguousarray(X_train)\n",
    "        \n",
    "        # Train (not needed for LSH but FAISS requires it)\n",
    "        start = time.time()\n",
    "        self.index.train(X_train)\n",
    "        train_time = time.time() - start\n",
    "        \n",
    "        # Add vectors\n",
    "        start = time.time()\n",
    "        self.index.add(X_train)\n",
    "        add_time = time.time() - start\n",
    "        \n",
    "        print(f\"    Train time: {train_time:.2f}s\")\n",
    "        print(f\"    Add time: {add_time:.2f}s\")\n",
    "        print(f\"    Total vectors: {self.index.ntotal:,}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def query(self, X_test, k):\n",
    "        \"\"\"\n",
    "        Query for k nearest neighbors.\n",
    "        \n",
    "        Returns:\n",
    "            indices: (n_queries, k) array\n",
    "            distances: (n_queries, k) array\n",
    "        \"\"\"\n",
    "        # Ensure data is float32 and C-contiguous\n",
    "        if X_test.dtype != np.float32:\n",
    "            X_test = X_test.astype(np.float32)\n",
    "        if not X_test.flags['C_CONTIGUOUS']:\n",
    "            X_test = np.ascontiguousarray(X_test)\n",
    "        \n",
    "        # Search\n",
    "        distances, indices = self.index.search(X_test, k)\n",
    "        \n",
    "        return indices, distances\n",
    "    \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Estimate memory usage in MB\"\"\"\n",
    "        # FAISS binary codes: nbits per vector\n",
    "        n_vectors = self.index.ntotal\n",
    "        bits_per_vector = self.nbits\n",
    "        bytes_total = (n_vectors * bits_per_vector) / 8\n",
    "        \n",
    "        # Add overhead for hash table structure (rough estimate)\n",
    "        overhead = bytes_total * 0.1\n",
    "        \n",
    "        return (bytes_total + overhead) / (1024**2)\n",
    "\n",
    "\n",
    "def build_faiss_lsh_index(X_train, nbits):\n",
    "    \"\"\"Wrapper for evaluator\"\"\"\n",
    "    dim = X_train.shape[1]\n",
    "    index = FAISSLSHIndex(dim, nbits)\n",
    "    index.fit(X_train)\n",
    "    return index\n",
    "\n",
    "\n",
    "def query_faiss_lsh_index(index, X_test, k):\n",
    "    \"\"\"Wrapper for evaluator\"\"\"\n",
    "    return index.query(X_test, k)\n",
    "\n",
    "\n",
    "print(\"✓ FAISS LSH implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run LSH Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# Initialize dataset loader\n",
    "loader = DatasetLoader(data_dir=DATA_DIR)\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Print dataset info\n",
    "    print_dataset_info(dataset_name)\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        X_train, X_test = loader.load_dataset(\n",
    "            dataset_name,\n",
    "            n_train=N_TRAIN,\n",
    "            n_test=N_TEST\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Could not load {dataset_name} dataset.\")\n",
    "        print(f\"  {e}\")\n",
    "        print(f\"  Skipping this dataset...\\n\")\n",
    "        continue\n",
    "    \n",
    "    d = X_train.shape[1]\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ANNEvaluator(X_train, X_test, k=K)\n",
    "    \n",
    "    # Compute ground truth\n",
    "    evaluator.compute_ground_truth()\n",
    "    \n",
    "    # Store results for this dataset\n",
    "    dataset_results = []\n",
    "    \n",
    "    # Sweep over hash bits\n",
    "    for nbits in HASH_BITS_OPTIONS:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating: FAISS LSH (nbits={nbits})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create index builder\n",
    "        def build_fn(X):\n",
    "            return build_faiss_lsh_index(X, nbits)\n",
    "        \n",
    "        # Evaluate\n",
    "        try:\n",
    "            results = evaluator.evaluate(\n",
    "                index_builder=build_fn,\n",
    "                query_func=query_faiss_lsh_index,\n",
    "                method_name=f\"FAISS-LSH-{dataset_name.upper()}-{nbits}bits\"\n",
    "            )\n",
    "            \n",
    "            # Add metadata\n",
    "            results['dataset'] = dataset_name\n",
    "            results['d'] = d\n",
    "            results['lsh_nbits'] = nbits\n",
    "            results['library'] = 'FAISS'\n",
    "            \n",
    "            dataset_results.append(results)\n",
    "            \n",
    "            # Save individual result\n",
    "            result_path = os.path.join(RESULTS_DIR, f\"lsh_faiss_{dataset_name}_{nbits}bits.json\")\n",
    "            with open(result_path, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            \n",
    "            print(f\"\\n✓ Results saved to {result_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during evaluation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Store results for this dataset\n",
    "    all_results[dataset_name] = dataset_results\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ALL LSH EXPERIMENTS COMPLETE\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create per-dataset plots\n",
    "for dataset_name, results_list in all_results.items():\n",
    "    if not results_list:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nPlotting results for {dataset_name.upper()}...\")\n",
    "    \n",
    "    # Extract data\n",
    "    nbits_list = [r['lsh_nbits'] for r in results_list]\n",
    "    recalls = [r['recall@k'] for r in results_list]\n",
    "    query_times = [r['avg_query_time_ms'] for r in results_list]\n",
    "    memories = [r['memory_mb'] for r in results_list]\n",
    "    build_times = [r['build_time_s'] for r in results_list]\n",
    "    d = results_list[0]['d']\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'FAISS LSH Results: {dataset_name.upper()} (d={d})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Recall vs Hash Bits\n",
    "    axes[0, 0].plot(nbits_list, recalls, marker='o', linewidth=2.5, markersize=10, color='steelblue')\n",
    "    axes[0, 0].axhline(y=0.90, color='orange', linestyle='--', alpha=0.5, label='90% Target')\n",
    "    axes[0, 0].set_xlabel('Number of Hash Bits', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel(f'Recall@{K}', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_title('Recall vs. Hash Bits', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xscale('log', base=2)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Annotate good configs\n",
    "    for i, (nb, r) in enumerate(zip(nbits_list, recalls)):\n",
    "        if r >= 0.90:\n",
    "            axes[0, 0].annotate(f'{r:.2f}', (nb, r), textcoords=\"offset points\", \n",
    "                               xytext=(0,5), ha='center', fontsize=9, color='green', fontweight='bold')\n",
    "    \n",
    "    # 2. Query Time vs Hash Bits\n",
    "    axes[0, 1].plot(nbits_list, query_times, marker='s', linewidth=2.5, markersize=10, color='coral')\n",
    "    axes[0, 1].set_xlabel('Number of Hash Bits', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Query Time (ms)', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_title('Query Time vs. Hash Bits', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xscale('log', base=2)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Memory Usage vs Hash Bits\n",
    "    axes[1, 0].plot(nbits_list, memories, marker='^', linewidth=2.5, markersize=10, \n",
    "                   color='mediumseagreen')\n",
    "    axes[1, 0].set_xlabel('Number of Hash Bits', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Memory (MB)', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_title('Memory Usage vs. Hash Bits', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xscale('log', base=2)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Recall-Query Time Pareto\n",
    "    scatter = axes[1, 1].scatter(query_times, recalls, s=150, c=nbits_list, \n",
    "                                cmap='viridis', alpha=0.7, edgecolors='black', linewidth=2,\n",
    "                                norm=plt.matplotlib.colors.LogNorm())\n",
    "    axes[1, 1].plot(query_times, recalls, 'k--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Annotate\n",
    "    for i, nb in enumerate(nbits_list):\n",
    "        axes[1, 1].annotate(f'{nb}', (query_times[i], recalls[i]), \n",
    "                           textcoords=\"offset points\", xytext=(5,5), ha='left', fontsize=9)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Query Time (ms)', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel(f'Recall@{K}', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].set_title('Pareto Frontier', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 1])\n",
    "    cbar.set_label('Hash Bits', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(RESULTS_DIR, f'lsh_faiss_{dataset_name}_tradeoffs.png')\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Plot saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all results\n",
    "all_results_flat = []\n",
    "for dataset_name, results_list in all_results.items():\n",
    "    all_results_flat.extend(results_list)\n",
    "\n",
    "if all_results_flat:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SUMMARY: FAISS LSH\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Header\n",
    "    print(f\"{'Dataset':<10} {'Hash Bits':<12} {'Recall@10':<12} {'Query(ms)':<12} \"\n",
    "          f\"{'Build(s)':<10} {'Memory(MB)':<12}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Sort by dataset, then nbits\n",
    "    sorted_results = sorted(all_results_flat, key=lambda x: (x['dataset'], x['lsh_nbits']))\n",
    "    \n",
    "    for r in sorted_results:\n",
    "        marker = \"✓\" if r['recall@k'] >= 0.90 else \" \"\n",
    "        \n",
    "        print(f\"{marker} {r['dataset'].upper():<9} {r['lsh_nbits']:<12} \"\n",
    "              f\"{r['recall@k']:<12.4f} {r['avg_query_time_ms']:<12.3f} \"\n",
    "              f\"{r['build_time_s']:<10.2f} {r['memory_mb']:<12.2f}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'experiment': 'LSH (FAISS)',\n",
    "        'k': K,\n",
    "        'n_test': N_TEST,\n",
    "        'library': 'faiss-cpu',\n",
    "        'index_type': 'IndexLSH',\n",
    "        'configurations_tested': len(all_results_flat),\n",
    "        'results_by_dataset': all_results\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(RESULTS_DIR, 'lsh_faiss_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Summary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST CONFIGURATIONS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for dataset_name, results_list in all_results.items():\n",
    "        if not results_list:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{dataset_name.upper()}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Best recall\n",
    "        best_recall = max(results_list, key=lambda x: x['recall@k'])\n",
    "        print(f\"\\n  Best Recall:\")\n",
    "        print(f\"    Hash bits: {best_recall['lsh_nbits']}\")\n",
    "        print(f\"    Recall@{K}: {best_recall['recall@k']:.4f}\")\n",
    "        print(f\"    Query time: {best_recall['avg_query_time_ms']:.3f} ms\")\n",
    "        \n",
    "        # Fastest with decent recall\n",
    "        decent_recall = [r for r in results_list if r['recall@k'] >= 0.80]\n",
    "        if decent_recall:\n",
    "            fastest = min(decent_recall, key=lambda x: x['avg_query_time_ms'])\n",
    "            print(f\"\\n  Fastest (Recall ≥ 0.80):\")\n",
    "            print(f\"    Hash bits: {fastest['lsh_nbits']}\")\n",
    "            print(f\"    Recall@{K}: {fastest['recall@k']:.4f}\")\n",
    "            print(f\"    Query time: {fastest['avg_query_time_ms']:.3f} ms\")\n",
    "        \n",
    "        # Most memory efficient with decent recall\n",
    "        if decent_recall:\n",
    "            mem_efficient = min(decent_recall, key=lambda x: x['memory_mb'])\n",
    "            print(f\"\\n  Most Memory Efficient (Recall ≥ 0.80):\")\n",
    "            print(f\"    Hash bits: {mem_efficient['lsh_nbits']}\")\n",
    "            print(f\"    Recall@{K}: {mem_efficient['recall@k']:.4f}\")\n",
    "            print(f\"    Memory: {mem_efficient['memory_mb']:.2f} MB\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Observations\n",
    "\n",
    "**FAISS Advantages:**\n",
    "- ✅ Extremely fast C++ implementation\n",
    "- ✅ Production-tested on billion-scale datasets\n",
    "- ✅ Memory efficient binary codes\n",
    "- ✅ SIMD-optimized distance computations\n",
    "\n",
    "**Expected Patterns:**\n",
    "- **More hash bits** → Better recall but more memory\n",
    "- **Fewer hash bits** → Faster queries but lower recall\n",
    "- Build time should be fast (just computing binary codes)\n",
    "- Query time should be sublinear\n",
    "\n",
    "**Comparison Points:**\n",
    "- Baseline KNN: Recall=1.0, but slower queries\n",
    "- JL+KNN: Good recall with dimensionality reduction\n",
    "- FAISS LSH: Should be fastest with acceptable recall\n",
    "\n",
    "**Next Steps:**\n",
    "1. Compare all methods (Baseline, JL, LSH)\n",
    "2. Create unified comparison plots\n",
    "3. Analyze which method works best for which dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
