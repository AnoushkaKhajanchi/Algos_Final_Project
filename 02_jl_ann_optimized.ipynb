{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Johnson-Lindenstrauss (JL) Dimensionality Reduction + ANN (Optimized)\n",
    "\n",
    "This notebook evaluates approximate nearest neighbor search using sklearn's optimized Johnson-Lindenstrauss random projection, followed by brute-force search in the lower-dimensional space.\n",
    "\n",
    "## Method\n",
    "1. Apply JL projection using sklearn.random_projection.SparseRandomProjection\n",
    "2. Build KNN index in k-dimensional space using sklearn\n",
    "3. Project query vectors and search in k-dimensional space\n",
    "\n",
    "## Key Advantages\n",
    "- ✅ Uses sklearn's highly optimized C/Cython implementation\n",
    "- ✅ Sparse random matrices (Achlioptas 2001) for efficiency\n",
    "- ✅ Automatic parameter selection based on JL lemma\n",
    "- ✅ Fast matrix operations with BLAS/LAPACK\n",
    "\n",
    "## Metrics\n",
    "- **Recall@10**: Proportion of true nearest neighbors retrieved\n",
    "- **Query time**: Time per query (including projection) in milliseconds\n",
    "- **Build time**: Time to construct projection + index\n",
    "- **Memory**: Total memory footprint in MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful!\n",
      "Using sklearn's optimized SparseRandomProjection\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Import our custom modules\n",
    "sys.path.append('..')  # If running from notebooks/ subdirectory\n",
    "from evaluator import ANNEvaluator\n",
    "from datasets import DatasetLoader, print_dataset_info\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "print(\"✓ Imports successful!\")\n",
    "print(f\"Using sklearn's optimized SparseRandomProjection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  k (neighbors) = 10\n",
      "  JL epsilon = 0.1\n",
      "  Datasets: ['sift', 'gist', 'deep1b']\n",
      "\n",
      "Target dimensions:\n",
      "  SIFT: [32, 64, 80, 90, 100, 120, 124]\n",
      "  GIST: [100, 200, 300, 500, 700]\n",
      "  DEEP1B: [32, 48, 64, 80]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "K = 10  # Number of nearest neighbors\n",
    "DATA_DIR = \"data\"  # Adjust if needed\n",
    "\n",
    "# Which datasets to evaluate\n",
    "DATASETS = [\n",
    "    'sift',\n",
    "    'gist',\n",
    "    'deep1b'\n",
    "]\n",
    "\n",
    "# Target dimensions - optimized based on dataset characteristics\n",
    "TARGET_DIMS = {\n",
    "    'sift': [32, 64, 80, 90, 100, 120, 124],           # 128-dim → 2-4x compression\n",
    "    'gist': [100, 200, 300, 500, 700],      # 960-dim → 1.4-10x compression\n",
    "    'deep1b': [32, 48, 64, 80]              # 96-dim → 1.2-3x compression\n",
    "}\n",
    "\n",
    "# Subset sizes\n",
    "N_TRAIN = None  # Use full training set\n",
    "N_TEST = 1000   # Use 1000 test queries\n",
    "\n",
    "# JL parameters for sklearn\n",
    "JL_EPS = 0.1        # Distance distortion parameter\n",
    "JL_DENSITY = 'auto' # Sparsity: 'auto' uses 1/sqrt(n_components)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Results directory\n",
    "RESULTS_DIR = \"../results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  k (neighbors) = {K}\")\n",
    "print(f\"  JL epsilon = {JL_EPS}\")\n",
    "print(f\"  Datasets: {DATASETS}\")\n",
    "print(f\"\\nTarget dimensions:\")\n",
    "for ds, dims in TARGET_DIMS.items():\n",
    "    print(f\"  {ds.upper()}: {dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sklearn-Based JL+KNN Implementation\n",
    "\n",
    "Using sklearn's production-ready implementations:\n",
    "- **SparseRandomProjection**: Optimized C/Cython implementation\n",
    "- **NearestNeighbors**: Fast brute-force search with parallel execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sklearn JL+KNN implementation ready!\n"
     ]
    }
   ],
   "source": [
    "class SklearnJLKNN:\n",
    "    \"\"\"\n",
    "    JL+KNN using sklearn's optimized implementations.\n",
    "    Much faster than custom implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components, eps=0.1, density='auto', n_jobs=-1, random_state=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_components: Target dimension k\n",
    "            eps: JL epsilon parameter for distance preservation\n",
    "            density: Sparsity of projection matrix ('auto' recommended)\n",
    "            n_jobs: Number of CPU cores (-1 = all)\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.eps = eps\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        # Initialize projector\n",
    "        self.projector = SparseRandomProjection(\n",
    "            n_components=n_components,\n",
    "            eps=eps,\n",
    "            density=density,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Initialize KNN\n",
    "        self.knn = NearestNeighbors(\n",
    "            algorithm='brute',\n",
    "            metric='euclidean',\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        \n",
    "        self.X_train_projected = None\n",
    "        \n",
    "    def fit(self, X_train):\n",
    "        \"\"\"Build projection and KNN index\"\"\"\n",
    "        print(f\"  Projecting {X_train.shape[0]:,} points: {X_train.shape[1]}D → {self.n_components}D...\")\n",
    "        \n",
    "        # Project training data (sklearn handles this efficiently)\n",
    "        start = time.time()\n",
    "        self.X_train_projected = self.projector.fit_transform(X_train)\n",
    "        proj_time = time.time() - start\n",
    "        \n",
    "        print(f\"    Projection time: {proj_time:.2f}s\")\n",
    "        print(f\"    Projected shape: {self.X_train_projected.shape}\")\n",
    "        print(f\"    Memory: {self.X_train_projected.nbytes / (1024**2):.2f} MB\")\n",
    "        \n",
    "        # Build KNN index on projected data\n",
    "        start = time.time()\n",
    "        self.knn.fit(self.X_train_projected)\n",
    "        knn_time = time.time() - start\n",
    "        \n",
    "        print(f\"    KNN index time: {knn_time:.2f}s\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def query(self, X_test, k):\n",
    "        \"\"\"Query KNN in projected space\"\"\"\n",
    "        # Project queries (fast with sklearn)\n",
    "        X_test_projected = self.projector.transform(X_test)\n",
    "        \n",
    "        # Query KNN\n",
    "        distances, indices = self.knn.kneighbors(X_test_projected, n_neighbors=k)\n",
    "        \n",
    "        return indices, distances\n",
    "    \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Estimate memory usage in MB\"\"\"\n",
    "        # Projected data\n",
    "        data_mem = self.X_train_projected.nbytes / (1024**2)\n",
    "        \n",
    "        # Projection matrix (sparse)\n",
    "        if hasattr(self.projector, 'components_'):\n",
    "            proj_mem = self.projector.components_.data.nbytes / (1024**2)\n",
    "        else:\n",
    "            proj_mem = 0\n",
    "        \n",
    "        return data_mem + proj_mem\n",
    "\n",
    "\n",
    "def build_sklearn_jl_index(X_train, n_components, eps=0.1, n_jobs=-1):\n",
    "    \"\"\"Wrapper for evaluator\"\"\"\n",
    "    model = SklearnJLKNN(\n",
    "        n_components=n_components,\n",
    "        eps=eps,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    model.fit(X_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def query_sklearn_jl_index(index, X_test, k):\n",
    "    \"\"\"Wrapper for evaluator\"\"\"\n",
    "    return index.query(X_test, k)\n",
    "\n",
    "\n",
    "print(\"✓ Sklearn JL+KNN implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run JL Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET: SIFT\n",
      "======================================================================\n",
      "\n",
      "SIFT-1M Dataset Info:\n",
      "  Description: SIFT image descriptors\n",
      "  Dimension: 128\n",
      "  Training size: 1,000,000\n",
      "  Test size: 10,000\n",
      "\n",
      "Loading SIFT-1M dataset...\n",
      "  Training: (1000000, 128) (dtype: float32)\n",
      "  Test: (1000, 128) (dtype: float32)\n",
      "Evaluator initialized:\n",
      "  Training points: 1,000,000\n",
      "  Test queries: 1,000\n",
      "  Dimensions: 128\n",
      "  k (neighbors): 10\n",
      "Computing ground truth k-NN (k=10) using brute force...\n",
      "Ground truth computed in 2.18s\n",
      "Ground truth shape: (1000, 10)\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN (d=128 → k=32)\n",
      "  Compression ratio: 4.00x\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN-SIFT-k32\n",
      "============================================================\n",
      "Building index...\n",
      "  Projecting 1,000,000 points: 128D → 32D...\n",
      "    Projection time: 0.59s\n",
      "    Projected shape: (1000000, 32)\n",
      "    Memory: 122.07 MB\n",
      "    KNN index time: 0.04s\n",
      "  Build time: 0.63s\n",
      "  Memory used: 244.14 MB\n",
      "Querying 1,000 test points...\n",
      "  Avg query time: 0.982 ms\n",
      "  Recall@10: 0.1800 (18.00%)\n",
      "  Recall stats: mean=0.1800, std=0.1673, min=0.0000, max=0.9000\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Results saved to ../results/jl_sklearn_sift_k32.json\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN (d=128 → k=64)\n",
      "  Compression ratio: 2.00x\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN-SIFT-k64\n",
      "============================================================\n",
      "Building index...\n",
      "  Projecting 1,000,000 points: 128D → 64D...\n",
      "    Projection time: 0.70s\n",
      "    Projected shape: (1000000, 64)\n",
      "    Memory: 244.14 MB\n",
      "    KNN index time: 0.09s\n",
      "  Build time: 0.79s\n",
      "  Memory used: 488.28 MB\n",
      "Querying 1,000 test points...\n",
      "  Avg query time: 1.476 ms\n",
      "  Recall@10: 0.3233 (32.33%)\n",
      "  Recall stats: mean=0.3233, std=0.1930, min=0.0000, max=1.0000\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Results saved to ../results/jl_sklearn_sift_k64.json\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN (d=128 → k=80)\n",
      "  Compression ratio: 1.60x\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN-SIFT-k80\n",
      "============================================================\n",
      "Building index...\n",
      "  Projecting 1,000,000 points: 128D → 80D...\n",
      "    Projection time: 0.63s\n",
      "    Projected shape: (1000000, 80)\n",
      "    Memory: 305.18 MB\n",
      "    KNN index time: 0.05s\n",
      "  Build time: 0.68s\n",
      "  Memory used: 0.01 MB\n",
      "Querying 1,000 test points...\n",
      "  Avg query time: 1.626 ms\n",
      "  Recall@10: 0.3818 (38.18%)\n",
      "  Recall stats: mean=0.3818, std=0.1862, min=0.0000, max=0.9000\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Results saved to ../results/jl_sklearn_sift_k80.json\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN (d=128 → k=90)\n",
      "  Compression ratio: 1.42x\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN-SIFT-k90\n",
      "============================================================\n",
      "Building index...\n",
      "  Projecting 1,000,000 points: 128D → 90D...\n",
      "    Projection time: 0.63s\n",
      "    Projected shape: (1000000, 90)\n",
      "    Memory: 343.32 MB\n",
      "    KNN index time: 0.05s\n",
      "  Build time: 0.68s\n",
      "  Memory used: 0.00 MB\n",
      "Querying 1,000 test points...\n",
      "  Avg query time: 1.752 ms\n",
      "  Recall@10: 0.3955 (39.55%)\n",
      "  Recall stats: mean=0.3955, std=0.1944, min=0.0000, max=1.0000\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Results saved to ../results/jl_sklearn_sift_k90.json\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN (d=128 → k=100)\n",
      "  Compression ratio: 1.28x\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN-SIFT-k100\n",
      "============================================================\n",
      "Building index...\n",
      "  Projecting 1,000,000 points: 128D → 100D...\n",
      "    Projection time: 0.70s\n",
      "    Projected shape: (1000000, 100)\n",
      "    Memory: 381.47 MB\n",
      "    KNN index time: 0.06s\n",
      "  Build time: 0.75s\n",
      "  Memory used: 0.04 MB\n",
      "Querying 1,000 test points...\n",
      "  Avg query time: 2.080 ms\n",
      "  Recall@10: 0.4251 (42.51%)\n",
      "  Recall stats: mean=0.4251, std=0.1888, min=0.0000, max=0.9000\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Results saved to ../results/jl_sklearn_sift_k100.json\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN (d=128 → k=120)\n",
      "  Compression ratio: 1.07x\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN-SIFT-k120\n",
      "============================================================\n",
      "Building index...\n",
      "  Projecting 1,000,000 points: 128D → 120D...\n",
      "    Projection time: 0.67s\n",
      "    Projected shape: (1000000, 120)\n",
      "    Memory: 457.76 MB\n",
      "    KNN index time: 0.06s\n",
      "  Build time: 0.73s\n",
      "  Memory used: 0.00 MB\n",
      "Querying 1,000 test points...\n",
      "  Avg query time: 2.165 ms\n",
      "  Recall@10: 0.4693 (46.93%)\n",
      "  Recall stats: mean=0.4693, std=0.1870, min=0.0000, max=1.0000\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Results saved to ../results/jl_sklearn_sift_k120.json\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN (d=128 → k=124)\n",
      "  Compression ratio: 1.03x\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN-SIFT-k124\n",
      "============================================================\n",
      "Building index...\n",
      "  Projecting 1,000,000 points: 128D → 124D...\n",
      "    Projection time: 0.74s\n",
      "    Projected shape: (1000000, 124)\n",
      "    Memory: 473.02 MB\n",
      "    KNN index time: 0.07s\n",
      "  Build time: 0.81s\n",
      "  Memory used: 0.00 MB\n",
      "Querying 1,000 test points...\n",
      "  Avg query time: 2.467 ms\n",
      "  Recall@10: 0.4829 (48.29%)\n",
      "  Recall stats: mean=0.4829, std=0.1806, min=0.0000, max=1.0000\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Results saved to ../results/jl_sklearn_sift_k124.json\n",
      "\n",
      "======================================================================\n",
      "DATASET: GIST\n",
      "======================================================================\n",
      "\n",
      "GIST-1M Dataset Info:\n",
      "  Description: GIST global image features\n",
      "  Dimension: 960\n",
      "  Training size: 1,000,000\n",
      "  Test size: 1,000\n",
      "\n",
      "Loading GIST-1M dataset...\n",
      "  Training: (1000000, 960) (dtype: float32)\n",
      "  Test: (1000, 960) (dtype: float32)\n",
      "Evaluator initialized:\n",
      "  Training points: 1,000,000\n",
      "  Test queries: 1,000\n",
      "  Dimensions: 960\n",
      "  k (neighbors): 10\n",
      "Computing ground truth k-NN (k=10) using brute force...\n",
      "Ground truth computed in 14.99s\n",
      "Ground truth shape: (1000, 10)\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN (d=960 → k=100)\n",
      "  Compression ratio: 9.60x\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: JL-KNN-GIST-k100\n",
      "============================================================\n",
      "Building index...\n",
      "  Projecting 1,000,000 points: 960D → 100D...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     results = evaluator.evaluate(\n\u001b[32m     57\u001b[39m         index_builder=build_fn,\n\u001b[32m     58\u001b[39m         query_func=query_sklearn_jl_index,\n\u001b[32m     59\u001b[39m         method_name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mJL-KNN-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     60\u001b[39m     )\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# Add metadata\u001b[39;00m\n\u001b[32m     63\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m] = dataset_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/Fall 2025/AlgosMassiveDataProject/Algos_Final_Project/evaluator.py:120\u001b[39m, in \u001b[36mANNEvaluator.evaluate\u001b[39m\u001b[34m(self, index_builder, query_func, method_name, **build_params)\u001b[39m\n\u001b[32m    117\u001b[39m mem_before = process.memory_info().rss / \u001b[32m1024\u001b[39m**\u001b[32m2\u001b[39m  \u001b[38;5;66;03m# MB\u001b[39;00m\n\u001b[32m    119\u001b[39m build_start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m index = index_builder(\u001b[38;5;28mself\u001b[39m.X_train, **build_params)\n\u001b[32m    121\u001b[39m build_time = time.time() - build_start\n\u001b[32m    123\u001b[39m mem_after = process.memory_info().rss / \u001b[32m1024\u001b[39m**\u001b[32m2\u001b[39m  \u001b[38;5;66;03m# MB\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mbuild_fn\u001b[39m\u001b[34m(X)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_fn\u001b[39m(X):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m build_sklearn_jl_index(X, target_dim, eps=JL_EPS)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mbuild_sklearn_jl_index\u001b[39m\u001b[34m(X_train, n_components, eps, n_jobs)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Wrapper for evaluator\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m model = SklearnJLKNN(\n\u001b[32m     85\u001b[39m     n_components=n_components,\n\u001b[32m     86\u001b[39m     eps=eps,\n\u001b[32m     87\u001b[39m     n_jobs=n_jobs,\n\u001b[32m     88\u001b[39m     random_state=RANDOM_STATE\n\u001b[32m     89\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m model.fit(X_train)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mSklearnJLKNN.fit\u001b[39m\u001b[34m(self, X_train)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Project training data (sklearn handles this efficiently)\u001b[39;00m\n\u001b[32m     41\u001b[39m start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28mself\u001b[39m.X_train_projected = \u001b[38;5;28mself\u001b[39m.projector.fit_transform(X_train)\n\u001b[32m     43\u001b[39m proj_time = time.time() - start\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Projection time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproj_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/algos/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = f(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/algos/lib/python3.11/site-packages/sklearn/base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/algos/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = f(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/algos/lib/python3.11/site-packages/sklearn/random_projection.py:824\u001b[39m, in \u001b[36mSparseRandomProjection.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    815\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    816\u001b[39m X = validate_data(\n\u001b[32m    817\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    818\u001b[39m     X,\n\u001b[32m   (...)\u001b[39m\u001b[32m    821\u001b[39m     dtype=[np.float64, np.float32],\n\u001b[32m    822\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m.components_.T, dense_output=\u001b[38;5;28mself\u001b[39m.dense_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/algos/lib/python3.11/site-packages/sklearn/utils/extmath.py:203\u001b[39m, in \u001b[36msafe_sparse_dot\u001b[39m\u001b[34m(a, b, dense_output)\u001b[39m\n\u001b[32m    201\u001b[39m         ret = xp.tensordot(a, b, axes=[-\u001b[32m1\u001b[39m, b_axis])\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     ret = a @ b\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    206\u001b[39m     sparse.issparse(a)\n\u001b[32m    207\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m sparse.issparse(b)\n\u001b[32m    208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[32m    209\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[33m\"\u001b[39m\u001b[33mtoarray\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    210\u001b[39m ):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.toarray()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/algos/lib/python3.11/site-packages/scipy/sparse/_base.py:911\u001b[39m, in \u001b[36m_spbase.__rmatmul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[32m    909\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mScalar operands are not allowed, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    910\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33muse \u001b[39m\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._rmatmul_dispatch(other)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/algos/lib/python3.11/site-packages/scipy/sparse/_base.py:892\u001b[39m, in \u001b[36m_spbase._rmatmul_dispatch\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m    891\u001b[39m     tr = np.asarray(other).transpose()\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m ret = \u001b[38;5;28mself\u001b[39m.transpose()._matmul_dispatch(tr)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/algos/lib/python3.11/site-packages/scipy/sparse/_base.py:797\u001b[39m, in \u001b[36m_spbase._matmul_dispatch\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    795\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape(M, \u001b[32m1\u001b[39m)\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m other.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other.shape[\u001b[32m0\u001b[39m] == N:\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._matmul_multivector(other)\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[32m    800\u001b[39m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[32m    801\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mul_scalar(other)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/algos/lib/python3.11/site-packages/scipy/sparse/_compressed.py:409\u001b[39m, in \u001b[36m_cs_matrix._matmul_multivector\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;66;03m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[32m    407\u001b[39m fn = \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m.format + \u001b[33m'\u001b[39m\u001b[33m_matvecs\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    408\u001b[39m fn(M, N, n_vecs, \u001b[38;5;28mself\u001b[39m.indptr, \u001b[38;5;28mself\u001b[39m.indices, \u001b[38;5;28mself\u001b[39m.data,\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m    other.ravel(), result.ravel())\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape((n_vecs,))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# Initialize dataset loader\n",
    "loader = DatasetLoader(data_dir=DATA_DIR)\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Print dataset info\n",
    "    print_dataset_info(dataset_name)\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        X_train, X_test = loader.load_dataset(\n",
    "            dataset_name,\n",
    "            n_train=N_TRAIN,\n",
    "            n_test=N_TEST\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Could not load {dataset_name} dataset.\")\n",
    "        print(f\"  {e}\")\n",
    "        print(f\"  Skipping this dataset...\\n\")\n",
    "        continue\n",
    "    \n",
    "    d_original = X_train.shape[1]\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ANNEvaluator(X_train, X_test, k=K)\n",
    "    \n",
    "    # Compute ground truth\n",
    "    evaluator.compute_ground_truth()\n",
    "    \n",
    "    # Store results for this dataset\n",
    "    dataset_results = []\n",
    "    \n",
    "    # Sweep over target dimensions\n",
    "    for target_dim in TARGET_DIMS[dataset_name]:\n",
    "        if target_dim >= d_original:\n",
    "            print(f\"\\nSkipping k={target_dim} (≥ original dimension {d_original})\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating: JL-KNN (d={d_original} → k={target_dim})\")\n",
    "        print(f\"  Compression ratio: {d_original/target_dim:.2f}x\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create index builder\n",
    "        def build_fn(X):\n",
    "            return build_sklearn_jl_index(X, target_dim, eps=JL_EPS)\n",
    "        \n",
    "        # Evaluate\n",
    "        try:\n",
    "            results = evaluator.evaluate(\n",
    "                index_builder=build_fn,\n",
    "                query_func=query_sklearn_jl_index,\n",
    "                method_name=f\"JL-KNN-{dataset_name.upper()}-k{target_dim}\"\n",
    "            )\n",
    "            \n",
    "            # Add metadata\n",
    "            results['dataset'] = dataset_name\n",
    "            results['d_original'] = d_original\n",
    "            results['k_projected'] = target_dim\n",
    "            results['compression_ratio'] = d_original / target_dim\n",
    "            results['jl_epsilon'] = JL_EPS\n",
    "            \n",
    "            dataset_results.append(results)\n",
    "            \n",
    "            # Save individual result\n",
    "            result_path = os.path.join(RESULTS_DIR, f\"jl_sklearn_{dataset_name}_k{target_dim}.json\")\n",
    "            with open(result_path, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            \n",
    "            print(f\"\\n✓ Results saved to {result_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during evaluation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Store results for this dataset\n",
    "    all_results[dataset_name] = dataset_results\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ALL JL EXPERIMENTS COMPLETE\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create per-dataset plots\n",
    "for dataset_name, results_list in all_results.items():\n",
    "    if not results_list:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nPlotting results for {dataset_name.upper()}...\")\n",
    "    \n",
    "    # Extract data\n",
    "    target_dims = [r['k_projected'] for r in results_list]\n",
    "    recalls = [r['recall@k'] for r in results_list]\n",
    "    query_times = [r['avg_query_time_ms'] for r in results_list]\n",
    "    memories = [r['memory_mb'] for r in results_list]\n",
    "    compression_ratios = [r['compression_ratio'] for r in results_list]\n",
    "    d_orig = results_list[0]['d_original']\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'JL+KNN Results (sklearn): {dataset_name.upper()} (d={d_orig})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Recall vs Target Dimension\n",
    "    axes[0, 0].plot(target_dims, recalls, marker='o', linewidth=2.5, markersize=10, color='steelblue')\n",
    "    axes[0, 0].axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='Perfect Recall')\n",
    "    axes[0, 0].axhline(y=0.90, color='orange', linestyle='--', alpha=0.5, label='90% Recall Target')\n",
    "    axes[0, 0].set_xlabel('Target Dimension k', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel(f'Recall@{K}', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_title('Recall vs. Dimension', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].set_ylim([max(0.5, min(recalls) - 0.05), 1.02])\n",
    "    \n",
    "    # Add annotations\n",
    "    for i, (k, r) in enumerate(zip(target_dims, recalls)):\n",
    "        if r >= 0.90:\n",
    "            axes[0, 0].annotate(f'{r:.2f}', (k, r), textcoords=\"offset points\", \n",
    "                               xytext=(0,5), ha='center', fontsize=9, color='green', fontweight='bold')\n",
    "    \n",
    "    # 2. Query Time vs Target Dimension  \n",
    "    axes[0, 1].plot(target_dims, query_times, marker='s', linewidth=2.5, markersize=10, color='coral')\n",
    "    axes[0, 1].set_xlabel('Target Dimension k', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Query Time (ms)', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_title('Query Time vs. Dimension', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Compression Ratio vs Recall\n",
    "    axes[1, 0].plot(compression_ratios, recalls, marker='^', linewidth=2.5, markersize=10, \n",
    "                   color='mediumseagreen')\n",
    "    axes[1, 0].axhline(y=0.90, color='orange', linestyle='--', alpha=0.5, label='90% Target')\n",
    "    axes[1, 0].set_xlabel('Compression Ratio (d/k)', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel(f'Recall@{K}', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_title('Recall vs. Compression Ratio', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Recall-Query Time Pareto Frontier\n",
    "    scatter = axes[1, 1].scatter(query_times, recalls, s=150, c=target_dims, \n",
    "                                cmap='viridis', alpha=0.7, edgecolors='black', linewidth=2)\n",
    "    # Connect points\n",
    "    axes[1, 1].plot(query_times, recalls, 'k--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Annotate points\n",
    "    for i, k in enumerate(target_dims):\n",
    "        axes[1, 1].annotate(f'k={k}', (query_times[i], recalls[i]), \n",
    "                           textcoords=\"offset points\", xytext=(5,5), ha='left', fontsize=9)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Query Time (ms)', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel(f'Recall@{K}', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].set_title('Pareto Frontier: Recall vs Speed', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 1])\n",
    "    cbar.set_label('Target Dimension k', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(RESULTS_DIR, f'jl_sklearn_{dataset_name}_tradeoffs.png')\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Plot saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all results\n",
    "all_results_flat = []\n",
    "for dataset_name, results_list in all_results.items():\n",
    "    all_results_flat.extend(results_list)\n",
    "\n",
    "if all_results_flat:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SUMMARY: JL+KNN (sklearn optimized)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Header\n",
    "    print(f\"{'Dataset':<10} {'d→k':<15} {'Ratio':<8} {'Recall@10':<12} {'Query(ms)':<12} {'Memory(MB)':<12}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Sort by dataset, then target dimension\n",
    "    sorted_results = sorted(all_results_flat, key=lambda x: (x['dataset'], x['k_projected']))\n",
    "    \n",
    "    for r in sorted_results:\n",
    "        dim_str = f\"{r['d_original']}→{r['k_projected']}\"\n",
    "        ratio_str = f\"{r['compression_ratio']:.2f}x\"\n",
    "        recall_str = f\"{r['recall@k']:.4f}\"\n",
    "        query_str = f\"{r['avg_query_time_ms']:.3f}\"\n",
    "        mem_str = f\"{r['memory_mb']:.3f}\"\n",
    "        \n",
    "        # Highlight good results (recall >= 0.90)\n",
    "        marker = \"✓\" if r['recall@k'] >= 0.90 else \" \"\n",
    "        \n",
    "        print(f\"{marker} {r['dataset'].upper():<9} {dim_str:<15} {ratio_str:<8} \"\n",
    "              f\"{recall_str:<12} {query_str:<12} {mem_str:<12}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'experiment': 'JL + KNN (sklearn)',\n",
    "        'k': K,\n",
    "        'n_test': N_TEST,\n",
    "        'jl_epsilon': JL_EPS,\n",
    "        'library': 'sklearn.random_projection.SparseRandomProjection',\n",
    "        'configurations_tested': len(all_results_flat),\n",
    "        'results_by_dataset': all_results\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(RESULTS_DIR, 'jl_sklearn_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Summary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST CONFIGURATIONS (Target: Recall ≥ 0.90)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for dataset_name, results_list in all_results.items():\n",
    "        if not results_list:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{dataset_name.upper()}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Filter high-recall configs (>= 0.90)\n",
    "        high_recall = [r for r in results_list if r['recall@k'] >= 0.90]\n",
    "        \n",
    "        if high_recall:\n",
    "            # Best compression with high recall\n",
    "            best_compression = max(high_recall, key=lambda x: x['compression_ratio'])\n",
    "            print(f\"\\n  Best Compression (Recall ≥ 0.90):\")\n",
    "            print(f\"    {best_compression['d_original']}D → {best_compression['k_projected']}D \"\n",
    "                  f\"({best_compression['compression_ratio']:.2f}x compression)\")\n",
    "            print(f\"    Recall@{K}: {best_compression['recall@k']:.4f}\")\n",
    "            print(f\"    Query time: {best_compression['avg_query_time_ms']:.3f} ms\")\n",
    "            \n",
    "            # Fastest with high recall\n",
    "            fastest = min(high_recall, key=lambda x: x['avg_query_time_ms'])\n",
    "            print(f\"\\n  Fastest Query (Recall ≥ 0.90):\")\n",
    "            print(f\"    {fastest['d_original']}D → {fastest['k_projected']}D\")\n",
    "            print(f\"    Recall@{K}: {fastest['recall@k']:.4f}\")\n",
    "            print(f\"    Query time: {fastest['avg_query_time_ms']:.3f} ms\")\n",
    "        else:\n",
    "            print(f\"\\n  ⚠ No configurations achieved Recall ≥ 0.90\")\n",
    "            best = max(results_list, key=lambda x: x['recall@k'])\n",
    "            print(f\"    Best recall: {best['recall@k']:.4f} at k={best['k_projected']}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Observations\n",
    "\n",
    "**sklearn Implementation Advantages:**\n",
    "- ✅ Highly optimized C/Cython code\n",
    "- ✅ Sparse random matrices for memory efficiency\n",
    "- ✅ Parallel query execution\n",
    "- ✅ Automatic parameter validation\n",
    "\n",
    "**Expected Performance:**\n",
    "- High recall (≥0.95) should be achievable with moderate compression (2-3x)\n",
    "- Query time should scale with target dimension k\n",
    "- Best results on high-dimensional data (GIST)\n",
    "\n",
    "**Next Steps:**\n",
    "1. Compare against baseline brute-force KNN\n",
    "2. Compare against optimized LSH implementation\n",
    "3. Analyze when JL provides the best tradeoffs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
